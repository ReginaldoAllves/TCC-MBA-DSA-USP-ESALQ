ucec.all = dplyr::select(ucec.all,
-Patient.ID,
-all_of(c(2:6)))
View(ucec.all)
ucec.data = ucec.all
str(ucec.data)
ucec.data$msi = factor(ucec.data$msi,
levels = c(0,1),
labels = c("MSS", "MSI"))
set.seed(1)
index = createDataPartition(ucec.data$msi, p = .75, list = FALSE)
trainingSet = ucec.data[index,]
testSet = ucec.data[-index,]
rangeModel <- preProcess(trainingSet, method = "range")
trainingSet <- predict(rangeModel, newdata = trainingSet)
testSet <- predict(rangeModel, testSet)
Ctrl <- trainControl(
method = "repeatedcv",              # Repeated k-fold cross-validation
number = 5,                         # Number of K
repeats = 5,                        # Number of repeats
savePredictions = "final",          # Save predictions of the final model only
classProbs = TRUE,                  # Save probabilities
summaryFunction = twoClassSummary   # Obtain AUC, Sensibility, and Specificity
)
set.seed(1)
rf.model <- train(msi ~ .,
data = trainingSet,
method = "rf",
metric = "ROC",
trControl = Ctrl,
tuneLength = 10)
set.seed(1)
nb.model <- train(msi ~ .,
data = trainingSet,
method = "nb",
metric = "ROC",
trControl = Ctrl,
tuneLength = 10)
set.seed(1)
nn.model <- train(msi ~ .,
data = trainingSet,
method = "nnet",
metric = "ROC",
trControl = Ctrl,
tuneLength = 10)
set.seed(1)
lr.model <- train(msi ~ .,
data = trainingSet,
method = "glmStepAIC",
metric = "ROC",
family = "binomial",
trControl = Ctrl,
tuneLength = 10)
summary(lr.model)
rfVarImp.plot = plot(varImp(rf.model), main="Variable Importance with Random Forest")
nbVarImp.plot = plot(varImp(nb.model), main="Variable Importance with Naive Bayes")
nnVarImp.plot = plot(varImp(nn.model), main="Variable Importance with Neural Network")
lr.plot = plot(varImp(lr.model), main="Variable Importance with Logistic Regression")
rfVarImp.plot
rfPerf.plot = plot(rf.model, main = "Model AUC with Random Forest")
nbPerf.plot = plot(nb.model, main = "Model AUC with Naive Bayes")
nnPerf.plot = plot(nn.model, main = "Model AUC with Neural Network")
rfPerf.plot
nbPerf.plot
nnPerf.plot
rm(list = ls())
library('caret')             # Training, hyper parameter tuning and prediction
library('caretEnsemble')     # Ensemble of caret models
library('tidyverse')         # Data wrangling
library('skimr')             # Descriptive statistics
library('GGally')            # Descriptive statistics
library('randomForest')      # Required to build Random Forest models
library('gbm')               # Required to build Binomial Logistic Regression models
library('nnet')              # Required to build Neural Networks
library('klaR')              # Required to build Naive Bayes
library('ggcorrplot')        # Required to draw a correlation matrix
library('pROC')              # Required to draw ROC curves
ucec.all = read.table("./data/ucec.final.no.pol.txt",
fill = TRUE, sep = "\t", header = TRUE, quote = "")
ucec.all = dplyr::select(ucec.all, -Patient.ID)
str(ucec.data)
ucec.data$msi = factor(ucec.data$msi,
levels = c(0,1),
labels = c("MSS", "MSI"))
ucec.data = ucec.all
str(ucec.data)
ucec.data$msi = factor(ucec.data$msi,
levels = c(0,1),
labels = c("MSS", "MSI"))
# Molecular
ggpairs(data=ucec.data[,c(1:7, 14)], aes(colour=msi))
set.seed(1)
index = createDataPartition(ucec.data$msi, p = .75, list = FALSE)
trainingSet = ucec.data[index,]
testSet = ucec.data[-index,]
prop.table(table(ucec.data$msi))
prop.table(table(trainingSet$msi))
prop.table(table(testSet$msi))
rangeModel <- preProcess(trainingSet, method = "range")
trainingSet <- predict(rangeModel, newdata = trainingSet)
testSet <- predict(rangeModel, testSet)
Ctrl <- trainControl(
method = "repeatedcv",              # Repeated k-fold cross-validation
number = 5,                         # Number of K
repeats = 5,                        # Number of repeats
savePredictions = "final",          # Save predictions of the final model only
classProbs = TRUE,                  # Save probabilities
summaryFunction = twoClassSummary   # Obtain AUC, Sensibility, and Specificity
)
set.seed(1)
rf.all.model <- train(msi ~ .,
data = trainingSet,
method = "rf",
metric = "ROC",
trControl = Ctrl,
tuneLength = 10)
set.seed(1)
nb.all.model <- train(msi ~ .,
data = trainingSet,
method = "nb",
metric = "ROC",
trControl = Ctrl,
tuneLength = 10)
set.seed(1)
set.seed(1)
nn.all.model <- train(msi ~ .,
data = trainingSet,
method = "nnet",
metric = "ROC",
trControl = Ctrl,
tuneLength = 10)
set.seed(1)
lr.all.model <- train(msi ~ .,
data = trainingSet,
method = "glmStepAIC",
metric = "ROC",
family = "binomial",
trControl = Ctrl,
tuneLength = 10)
?saveRDS
saveRDS(rf.all.model, './classifiers/rf.all.model.rds')
saveRDS(nb.all.model, './classifiers/nb.all.model.rds')
saveRDS(nn.all.model, './classifiers/nn.all.model.rds')
saveRDS(lr.all.model, './classifiers/lr.all.model.rds')
rm(list = ls())
ucec.all = read.table("./data/ucec.final.no.pol.txt",
fill = TRUE, sep = "\t", header = TRUE, quote = "")
ucec.all = dplyr::select(ucec.all,
-Patient.ID,
-all_of(c(2:6)))
View(ucec.all)
ucec.data = ucec.all
str(ucec.data)
ucec.data$msi = factor(ucec.data$msi,
levels = c(0,1),
labels = c("MSS", "MSI"))
# Molecular
ggpairs(data=ucec.data[,c(1:7, 14)], aes(colour=msi))
# Substitutions
ggpairs(data=ucec.data[,c(8:14)], aes(colour=msi))
set.seed(1)
index = createDataPartition(ucec.data$msi, p = .75, list = FALSE)
trainingSet = ucec.data[index,]
testSet = ucec.data[-index,]
prop.table(table(ucec.data$msi))
prop.table(table(trainingSet$msi))
prop.table(table(testSet$msi))
rangeModel <- preProcess(trainingSet, method = "range")
trainingSet <- predict(rangeModel, newdata = trainingSet)
testSet <- predict(rangeModel, testSet)
Ctrl <- trainControl(
method = "repeatedcv",              # Repeated k-fold cross-validation
number = 5,                         # Number of K
repeats = 5,                        # Number of repeats
savePredictions = "final",          # Save predictions of the final model only
classProbs = TRUE,                  # Save probabilities
summaryFunction = twoClassSummary   # Obtain AUC, Sensibility, and Specificity
)
set.seed(1)
rf.min.model <- train(msi ~ .,
data = trainingSet,
method = "rf",
metric = "ROC",
trControl = Ctrl,
tuneLength = 10)
set.seed(1)
nb.min.model <- train(msi ~ .,
data = trainingSet,
method = "nb",
metric = "ROC",
trControl = Ctrl,
tuneLength = 10)
set.seed(1)
nn.min.model <- train(msi ~ .,
data = trainingSet,
method = "nnet",
metric = "ROC",
trControl = Ctrl,
tuneLength = 10)
lr.min.model <- train(msi ~ .,
data = trainingSet,
method = "glmStepAIC",
metric = "ROC",
family = "binomial",
trControl = Ctrl,
tuneLength = 10)
summary(lr.model)
saveRDS(rf.min.model, './classifiers/rf.min.model.rds')
saveRDS(nb.min.model, './classifiers/nb.min.model.rds')
saveRDS(nn.min.model, './classifiers/nn.min.model.rds')
saveRDS(lr.min.model, './classifiers/lr.min.model.rds')
rf.min.model
nb.min.model
library(klaR)
?klaR::NaiveBayes()
nn.min.model
?saveRDS
rf.all.model = readRDS('./classifiers/rf.all.model.rds')
nb.all.model = readRDS('./classifiers/nb.all.model.rds')
nn.all.model = readRDS('./classifiers/nn.all.model.rds')
lr.all.model = readRDS('./classifiers/lr.all.model.rds')
rf.all.mode
rf.all.model = readRDS('./classifiers/rf.all.model.rds')
rf.all.model
nb.all.model
nn.all.model
lr.min.model
summary(lr.all.model)
rm(list = ls())
library(klaR)
library('caret')             # Training, hyper parameter tuning and prediction
library('caretEnsemble')     # Ensemble of caret models
library('tidyverse')         # Data wrangling
library('skimr')             # Descriptive statistics
library('GGally')            # Descriptive statistics
library('randomForest')      # Required to build Random Forest models
library('gbm')               # Required to build Binomial Logistic Regression models
library('nnet')              # Required to build Neural Networks
library('klaR')              # Required to build Naive Bayes
library('ggcorrplot')        # Required to draw a correlation matrix
library('pROC')              # Required to draw ROC curves
ucec.all = read.table("./data/ucec.final.no.pol.txt",
fill = TRUE, sep = "\t", header = TRUE, quote = "")
ucec.all = dplyr::select(ucec.all, -Patient.ID)
ucec.all = dplyr::select(ucec.all,
-Patient.ID,
-all_of(c(6,7)))
rm(list = ls())
ucec.all = read.table("./data/ucec.final.no.pol.txt",
fill = TRUE, sep = "\t", header = TRUE, quote = "")
View(ucec.all)
ucec.all = dplyr::select(ucec.all, -Patient.ID)
sapply(ucec.all, anyNA)
any(duplicated(ucec.all))
ucec.data = ucec.all
str(ucec.data)
ucec.data$msi = factor(ucec.data$msi,
levels = c(0,1),
labels = c("MSS", "MSI"))
rangeModel <- preProcess(trainingSet, method = "range")
set.seed(1)
index = createDataPartition(ucec.data$msi, p = .75, list = FALSE)
trainingSet = ucec.data[index,]
testSet = ucec.data[-index,]
prop.table(table(ucec.data$msi))
prop.table(table(trainingSet$msi))
prop.table(table(testSet$msi))
rangeModel <- preProcess(trainingSet, method = "range")
trainingSet <- predict(rangeModel, newdata = trainingSet)
testSet <- predict(rangeModel, testSet)
Ctrl <- trainControl(
method = "repeatedcv",              # Repeated k-fold cross-validation
number = 5,                         # Number of K
repeats = 5,                        # Number of repeats
savePredictions = "final",          # Save predictions of the final model only
classProbs = TRUE,                  # Save probabilities
summaryFunction = twoClassSummary   # Obtain AUC, Sensibility, and Specificity
)
rm(list = ls())
library(caret)
rf.all.model = readRDS('./classifiers/rf.all.model.rds')
nb.all.model = readRDS('./classifiers/nb.all.model.rds')
nn.all.model = readRDS('./classifiers/nn.all.model.rds')
lr.all.model = readRDS('./classifiers/lr.all.model.rds')
rf.min.model = readRDS('./classifiers/rf.min.model.rds')
nb.min.model = readRDS('./classifiers/nb.min.model.rds')
nn.min.model = readRDS('./classifiers/nn.min.model.rds')
lr.min.model = readRDS('./classifiers/lr.min.model.rds')
summary(lr.all.model)
summary(lr.all.model)
summary(lr.min.model)
?Heatmap
library(tidyverse)
library("ComplexHeatmap")
?Heatmap
library('caret')             # Training, hyper parameter tuning and prediction
library('caretEnsemble')     # Ensemble of caret models
library('tidyverse')         # Data wrangling
library('skimr')             # Descriptive statistics
library('GGally')            # Descriptive statistics
library('randomForest')      # Required to build Random Forest models
library('gbm')               # Required to build Binomial Logistic Regression models
library('nnet')              # Required to build Neural Networks
library('klaR')              # Required to build Naive Bayes
library('ggcorrplot')        # Required to draw a correlation matrix
library('pROC')              # Required to draw ROC curves
ucec.all = read.table("./data/ucec.final.no.pol.txt",
fill = TRUE, sep = "\t", header = TRUE, quote = "")
rm(list = ls())
library('caret')             # Training, hyper parameter tuning and prediction
library('caretEnsemble')     # Ensemble of caret models
library('tidyverse')         # Data wrangling
library('skimr')             # Descriptive statistics
library('GGally')            # Descriptive statistics
library('randomForest')      # Required to build Random Forest models
library('gbm')               # Required to build Binomial Logistic Regression models
library('nnet')              # Required to build Neural Networks
library('klaR')              # Required to build Naive Bayes
library('ggcorrplot')        # Required to draw a correlation matrix
library('pROC')              # Required to draw ROC curves
rf.all.model = readRDS('./classifiers/rf.all.model.rds')
rf.all.model
nb.all.model
nb.all.model = readRDS('./classifiers/nb.all.model.rds')
nn.all.model = readRDS('./classifiers/nn.all.model.rds')
lr.all.model = readRDS('./classifiers/lr.all.model.rds')
nb.all.model
nn.all.model
?ggcorrplot
?modelCor
coad.all = read.table("./data/coad.final.no.pol.txt",
fill = TRUE, sep = "\t", header = TRUE, quote = "")
rm(list = ls())
library('caret')             # Training, hyper parameter tuning and prediction
library('caretEnsemble')     # Ensemble of caret models
library('tidyverse')         # Data wrangling
library('skimr')             # Descriptive statistics
library('GGally')            # Descriptive statistics
library('randomForest')      # Required to build Random Forest models
library('gbm')               # Required to build Binomial Logistic Regression models
library('nnet')              # Required to build Neural Networks
library('klaR')              # Required to build Naive Bayes
library('ggcorrplot')        # Required to draw a correlation matrix
library('pROC')              # Required to draw ROC curves
ucec.all = read.table("./data/ucec.final.no.pol.txt",
fill = TRUE, sep = "\t", header = TRUE, quote = "")
ucec.all = dplyr::select(ucec.all, -Patient.ID)
ucec.all = dplyr::select(ucec.all,
-Patient.ID,
-all_of(c(6,7)))
rm(list = ls())
ucec.all = read.table("./data/ucec.final.no.pol.txt",
fill = TRUE, sep = "\t", header = TRUE, quote = "")
ucec.all = dplyr::select(ucec.all, -Patient.ID)
str(ucec.data)
ucec.data = ucec.all
str(ucec.data)
ucec.data$msi = factor(ucec.data$msi,
levels = c(0,1),
labels = c("MSS", "MSI"))
set.seed(1)
index = createDataPartition(ucec.data$msi, p = .75, list = FALSE)
trainingSet = ucec.data[index,]
testSet = ucec.data[-index,]
prop.table(table(ucec.data$msi))
prop.table(table(trainingSet$msi))
prop.table(table(testSet$msi))
rangeModel <- preProcess(trainingSet, method = "range")
trainingSet <- predict(rangeModel, newdata = trainingSet)
testSet <- predict(rangeModel, testSet)
rf.all.model = readRDS('./classifiers/rf.all.model.rds')
nb.all.model = readRDS('./classifiers/nb.all.model.rds')
nn.all.model = readRDS('./classifiers/nn.all.model.rds')
lr.all.model = readRDS('./classifiers/lr.all.model.rds')
?varImp
fittedRF <- predict(rf.model, testSet)
models_compare <- resamples(list(
RandomForest=rf.all.model,
NaiveBayes=nb.all.model,
NeuralNetwork=nn.all.model,
LogisticRegression=lr.all.model))
summary(models_compare)
scales <- list(x=list(relation="free"), y=list(relation="free"))
summary(models_compare)
models_compare <- resamples(list(
RandomForest=rf.min.model,
NaiveBayes=nb.min.model,
NeuralNetwork=nn.min.model,
LogisticRegression=lr.min.model))
rf.min.model = readRDS('./classifiers/rf.min.model.rds')
nb.min.model = readRDS('./classifiers/nb.min.model.rds')
nn.min.model = readRDS('./classifiers/nn.min.model.rds')
lr.min.model = readRDS('./classifiers/lr.min.model.rds')
models_compare <- resamples(list(
RandomForest=rf.min.model,
NaiveBayes=nb.min.model,
NeuralNetwork=nn.min.model,
LogisticRegression=lr.min.model))
summary(models_compare)
min.models_compare <- resamples(list(
RandomForest=rf.min.model,
NaiveBayes=nb.min.model,
NeuralNetwork=nn.min.model,
LogisticRegression=lr.min.model))
all.models_compare <- resamples(list(
RandomForest=rf.all.model,
NaiveBayes=nb.all.model,
NeuralNetwork=nn.all.model,
LogisticRegression=lr.all.model))
summary(all.models_compare)
summary(min.models_compare)
library(caret)
library(pROC)
data(iris)
iris <- iris[iris$Species == "virginica" | iris$Species == "versicolor", ]
iris$Species <- factor(iris$Species)  # setosa should be removed from factor
iris
str(iris)
samples <- sample(NROW(iris), NROW(iris) * .5)
data.train <- iris[samples, ]
data.test <- iris[-samples, ]
forest.model <- train(Species ~., data.train)
forest.model
result.predicted.prob <- predict(forest.model, data.test, type="prob") # Prediction
result.predicted.prob
result.roc <- roc(data.test$Species, result.predicted.prob$versicolor) # Draw ROC curve.
plot(result.roc, print.thres="best", print.thres.best.method="closest.topleft")
result.coords <- coords(result.roc, "best", best.method="closest.topleft", ret=c("threshold", "accuracy"))
print(result.coords)#to get threshold and accuracy
result.roc
iris
coad.all = read.table("./data/coad.final.no.pol.txt",
fill = TRUE, sep = "\t", header = TRUE, quote = "")
View(coad.all)
coad.all = dplyr::select(coad.all, -1)
View(coad.all)
coad.data = coad.all
str(coad.data)
coad.data$msi = factor(coad.data$msi,
levels = c(0,1),
labels = c("MSS", "MSI"))
prop.table(table(coad.data$msi))
coad.rangeModel <- preProcess(coad.data, method = "range")
library('caret')             # Training, hyper parameter tuning and prediction
library('caretEnsemble')     # Ensemble of caret models
library('tidyverse')         # Data wrangling
library('skimr')             # Descriptive statistics
library('GGally')            # Descriptive statistics
library('randomForest')      # Required to build Random Forest models
library('gbm')               # Required to build Binomial Logistic Regression models
library('nnet')              # Required to build Neural Networks
library('klaR')              # Required to build Naive Bayes
library('ggcorrplot')        # Required to draw a correlation matrix
library('pROC')              # Required to draw ROC curves
coad.rangeModel <- preProcess(coad.data, method = "range")
coad.data <- predict(coad.rangeModel, newdata = coad.data)
rf.all.model = readRDS('./classifiers/rf.all.model.rds')
nb.all.model = readRDS('./classifiers/nb.all.model.rds')
nn.all.model = readRDS('./classifiers/nn.all.model.rds')
lr.all.model = readRDS('./classifiers/lr.all.model.rds')
rf.min.model = readRDS('./classifiers/rf.min.model.rds')
nb.min.model = readRDS('./classifiers/nb.min.model.rds')
nn.min.model = readRDS('./classifiers/nn.min.model.rds')
lr.min.model = readRDS('./classifiers/lr.min.model.rds')
prob.all.fittedRF <- predict(rf.all.model, coad.data, type = "prob")
prob.all.fittedNB <- predict(nb.all.model, coad.data, type = "prob")
prob.all.fittedNN <- predict(nn.all.model, coad.data, type = "prob")
prob.all.fittedLR <- predict(lr.all.model, coad.data, type = "prob")
prob.min.fittedRF <- predict(rf.min.model, coad.data, type = "prob")
prob.min.fittedNB <- predict(nb.min.model, coad.data, type = "prob")
prob.min.fittedNN <- predict(nn.min.model, coad.data, type = "prob")
prob.min.fittedLR <- predict(lr.min.model, coad.data, type = "prob")
roc.all.fittedRF <- roc(coad.data$msi, prob.all.fittedRF$MSS)
roc.all.fittedNB <- roc(coad.data$msi, prob.all.fittedNB$MSS)
roc.all.fittedNN <- roc(coad.data$msi, prob.all.fittedNN$MSS)
roc.all.fittedLR <- roc(coad.data$msi, prob.all.fittedLR$MSS)
roc.min.fittedRF <- roc(coad.data$msi, prob.min.fittedRF$MSS)
roc.min.fittedNB <- roc(coad.data$msi, prob.min.fittedNB$MSS)
roc.min.fittedNN <- roc(coad.data$msi, prob.min.fittedNN$MSS)
roc.min.fittedLR <- roc(coad.data$msi, prob.min.fittedLR$MSS)
roc.all.fittedRF
roc.all.fittedRF <- roc(coad.data$msi, prob.all.fittedRF$MSS)
roc.all.fittedRF
roc.all.fittedNB
roc.all.fittedNN
roc.all.fittedLR
roc.min.fittedRF
roc.min.fittedNB
roc.min.fittedNN
roc.min.fittedLR
rm(list = ls())
library('caret')             # Training, hyper parameter tuning and prediction
library('caretEnsemble')     # Ensemble of caret models
library('tidyverse')         # Data wrangling
library('skimr')             # Descriptive statistics
library('GGally')            # Descriptive statistics
library('randomForest')      # Required to build Random Forest models
library('gbm')               # Required to build Binomial Logistic Regression models
library('nnet')              # Required to build Neural Networks
library('klaR')              # Required to build Naive Bayes
library('ggcorrplot')        # Required to draw a correlation matrix
library('pROC')              # Required to draw ROC curves
rf.all.model = readRDS('./classifiers/rf.all.model.rds')
nb.all.model = readRDS('./classifiers/nb.all.model.rds')
nn.all.model = readRDS('./classifiers/nn.all.model.rds')
lr.all.model = readRDS('./classifiers/lr.all.model.rds')
nb.all.model
rf.min.model = readRDS('./classifiers/rf.min.model.rds')
nb.min.model = readRDS('./classifiers/nb.min.model.rds')
nn.min.model = readRDS('./classifiers/nn.min.model.rds')
lr.min.model = readRDS('./classifiers/lr.min.model.rds')
nb.min.model
